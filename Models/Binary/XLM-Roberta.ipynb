{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abdul\n",
      "[nltk_data]     Manaf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import helper as hlp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "MAX_LEN = 50\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-5\n",
    "NAME = 'xlm(roberta)'\n",
    "MODEL_NAME = 'xlm-roberta-base'\n",
    "dir = '../../../Dataset/Updated_Dataset'\n",
    "path = f'saved_models/{NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{dir}/train_data.csv')\n",
    "val_df = pd.read_csv(f'{dir}/val_data.csv')\n",
    "test_df = pd.read_csv(f'{dir}/test_data.csv')\n",
    "\n",
    "train_texts = train_df['tweet'].values\n",
    "val_texts = val_df['tweet'].values\n",
    "test_texts = test_df['tweet'].values\n",
    "\n",
    "train_labels = train_df['label'].values\n",
    "val_labels = val_df['label'].values\n",
    "test_labels = test_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first batch of invitations to the power write course be go out now spring21 batch if you get an email from ben you be in 0\n",
      "do not make me do the emotional labor of answer how be you 1\n",
      "in politicalmedical news there be an impact stool in our body politic 0\n",
      "there be no product in sf 0\n",
      "dad look bianca it be the first time you dress up like an anime girl me die inside 0\n"
     ]
    }
   ],
   "source": [
    "#show 5 values along with their labels\n",
    "for i in range(5):\n",
    "    print(train_texts[i], train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first batch invitations power write course go spring21 batch get email ben 0\n",
      "make emotional labor answer 1\n",
      "politicalmedical news impact stool body politic 0\n",
      "product sf 0\n",
      "dad look bianca first time dress like anime girl die inside 0\n"
     ]
    }
   ],
   "source": [
    "#show 5 values along with their labels\n",
    "for i in range(5):\n",
    "    print(hlp.preprocess_text(train_texts[i])\n",
    "    , train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply preprocessing to all texts\n",
    "train_texts = [hlp.preprocess_text(text) for text in train_texts]\n",
    "val_texts = [hlp.preprocess_text(text) for text in val_texts]\n",
    "test_texts = [hlp.preprocess_text(text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## mbert model and tokenizer\n",
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 85/85 [00:40<00:00,  2.10it/s, loss=0.431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5392403202898363 accuracy 0.7083563918096292\n",
      "Validation loss 0.46342222627840546 accuracy 0.7598967297762479\n",
      "Model and tokenizer saved at epoch 1\n",
      "Checkpoint saved: Improved validation accuracy at epoch 1: 0.7598967297762479\n",
      "Metrics logged at epoch 1\n",
      "Epoch 2/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 85/85 [00:38<00:00,  2.23it/s, loss=0.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.39412929468295155 accuracy 0.8260468548238332\n"
     ]
    }
   ],
   "source": [
    "train_data_loader = hlp.create_data_loader(train_texts, train_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = hlp.create_data_loader(val_texts, val_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "history = hlp.train_model(train_data_loader, val_data_loader, model, optimizer, device, EPOCHS, tokenizer, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def extract_epoch_number(dir):\n",
    "    files = os.listdir(dir)\n",
    "    epoch = 0\n",
    "    for file in files:\n",
    "        if 'model_epoch' in file:\n",
    "            epoch = max(epoch, int(file.split('_')[-1]))\n",
    "    return epoch\n",
    "\n",
    "path = f'../saved_models/{NAME}'\n",
    "epoch = extract_epoch_number(path)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'{path}/tokenizer')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(f'{path}/model_epoch_{epoch}', num_labels=N_CLASSES)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = hlp.create_data_loader(test_texts, test_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "test_acc, test_loss, test_label_actual, test_label_pred = hlp.eval_model(model, test_data_loader, device)\n",
    "\n",
    "print(f'Test Accuracy: {test_acc} Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp.plot_confusion_matrix(test_label_actual, test_label_pred, ['Non-Depression', 'Depression'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
